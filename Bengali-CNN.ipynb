{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "import gc\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data files\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Preprocessing functions. Credit to user iafoss\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 128\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=SIZE, pad=16):\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df of labels\n",
    "trainlbl = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\n",
    "len(trainlbl['consonant_diacritic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess training data\n",
    "for i in range(3,4):\n",
    "    data = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_' + str(i) + '.parquet')\n",
    "    data = 255 - data.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "    resized = []\n",
    "    for idx in range(len(data)):\n",
    "        img = (data[idx]*(255.0/data[idx].max())).astype(np.uint8)\n",
    "        resized.append(crop_resize(img))\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train = np.array(resized).reshape(len(resized),SIZE,SIZE,1)\n",
    "    del resized\n",
    "    gc.collect()\n",
    "    with open('train_pre_' + str(i), 'wb') as gen_save:\n",
    "        pickle.dump(train, gen_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for lbl1\n",
    "densenodes = 128\n",
    "kernsize = 3\n",
    "poolsize = 2\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Conv2D(64, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(densenodes, activation=tf.nn.relu),\n",
    "tf.keras.layers.Dense(168,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model1.compile(optimizer=eval('tf.keras.optimizers.Adam(lr=0.001)'), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for lbl2\n",
    "densenodes = 128\n",
    "kernsize = 3\n",
    "poolsize = 2\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Conv2D(64, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(densenodes, activation=tf.nn.relu),\n",
    "tf.keras.layers.Dense(11,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=eval('tf.keras.optimizers.Adam(lr=0.001)'), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for lbl3\n",
    "densenodes = 128\n",
    "kernsize = 3\n",
    "poolsize = 2\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Conv2D(64, (kernsize,kernsize), padding='same', activation=tf.nn.relu, input_shape=(SIZE,SIZE,1)),\n",
    "tf.keras.layers.MaxPooling2D((poolsize, poolsize), strides=2),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(densenodes, activation=tf.nn.relu),\n",
    "tf.keras.layers.Dense(7,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model3.compile(optimizer=eval('tf.keras.optimizers.Adam(lr=0.001)'), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model 1\n",
    "for i in range(4):\n",
    "    with open ('/kaggle/input/train-pre/train_pre_' + str(i), 'rb') as train_load:\n",
    "        train = pickle.load(train_load)\n",
    "    num_train_examples = train.shape[0]\n",
    "    lblstart = num_train_examples * i\n",
    "    trainlbl = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv').iloc[lblstart:lblstart + num_train_examples,1]\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255).flow(train,trainlbl)\n",
    "    del train\n",
    "    del trainlbl\n",
    "    gc.collect()\n",
    "    model1.fit_generator(train_data_gen, epochs=10, steps_per_epoch=math.ceil(num_train_examples/batchsize), verbose=1)\n",
    "    del train_data_gen\n",
    "    gc.collect()\n",
    "model1.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model 2\n",
    "for i in range(4):\n",
    "    with open ('/kaggle/input/train-pre/train_pre_' + str(i), 'rb') as train_load:\n",
    "        train = pickle.load(train_load)\n",
    "    num_train_examples = train.shape[0]\n",
    "    lblstart = num_train_examples * i\n",
    "    trainlbl = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv').iloc[lblstart:lblstart + num_train_examples,2]\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255).flow(train,trainlbl)\n",
    "    del train\n",
    "    del trainlbl\n",
    "    gc.collect()\n",
    "    model2.fit_generator(train_data_gen, epochs=10, steps_per_epoch=math.ceil(num_train_examples/batchsize), verbose=1)\n",
    "    del train_data_gen\n",
    "    gc.collect()\n",
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model 3\n",
    "for i in range(4):\n",
    "    with open ('/kaggle/input/train-pre/train_pre_' + str(i), 'rb') as train_load:\n",
    "        train = pickle.load(train_load)\n",
    "    num_train_examples = train.shape[0]\n",
    "    lblstart = num_train_examples * i\n",
    "    trainlbl = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv').iloc[lblstart:lblstart + num_train_examples,3]\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255).flow(train,trainlbl)\n",
    "    del train\n",
    "    del trainlbl\n",
    "    gc.collect()\n",
    "    model3.fit_generator(train_data_gen, epochs=10, steps_per_epoch=math.ceil(num_train_examples/batchsize), verbose=1)\n",
    "    del train_data_gen\n",
    "    gc.collect()\n",
    "model3.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models\n",
    "model1 = tf.keras.models.load_model('/kaggle/input/models-test1/model1.h5')\n",
    "model2 = tf.keras.models.load_model('/kaggle/input/models-test1/model2.h5')\n",
    "model3 = tf.keras.models.load_model('/kaggle/input/models-test1/model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify test data\n",
    "out1 = []\n",
    "out2 = []\n",
    "out3 = []\n",
    "for i in range(4):\n",
    "    data = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_' + str(i) + '.parquet')\n",
    "    lbls = data.iloc[:,0]\n",
    "    data = 255 - data.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "    resized = []\n",
    "    for idx in range(len(data)):\n",
    "        img = (data[idx]*(255.0/data[idx].max())).astype(np.uint8)\n",
    "        resized.append(crop_resize(img))\n",
    "    del data\n",
    "    gc.collect()\n",
    "    test = np.array(resized).reshape(len(resized),SIZE,SIZE,1)\n",
    "    del resized\n",
    "    gc.collect()\n",
    "    #model1\n",
    "    predictions = model1.predict(test)\n",
    "    pred_list = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred_list.append(np.argmax(predictions[i]))\n",
    "    outdf = lbls.to_frame().rename(columns={'image_id':'row_id'})\n",
    "    outdf['target'] = pred_list\n",
    "    outdf['row_id'] = outdf['row_id'] + '_grapheme_root'\n",
    "    out1.append(outdf)\n",
    "    #model2\n",
    "    predictions = model2.predict(test)\n",
    "    pred_list = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred_list.append(np.argmax(predictions[i]))\n",
    "    outdf = lbls.to_frame().rename(columns={'image_id':'row_id'})\n",
    "    outdf['target'] = pred_list\n",
    "    outdf['row_id'] = outdf['row_id'] + '_vowel_diacritic'\n",
    "    out2.append(outdf)\n",
    "    #model3\n",
    "    predictions = model3.predict(test)\n",
    "    pred_list = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        pred_list.append(np.argmax(predictions[i]))\n",
    "    outdf = lbls.to_frame().rename(columns={'image_id':'row_id'})\n",
    "    outdf['target'] = pred_list\n",
    "    outdf['row_id'] = outdf['row_id'] + '_consonant_diacritic'\n",
    "    out3.append(outdf)\n",
    "out1 = pd.concat(out1)\n",
    "out2 = pd.concat(out2)\n",
    "out3 = pd.concat(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate submission\n",
    "submission = pd.concat([out1,out2,out3]).reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
